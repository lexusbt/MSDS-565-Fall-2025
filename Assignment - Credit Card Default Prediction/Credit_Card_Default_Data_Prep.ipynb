{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction: Data Preparation & Feature Engineering\n",
    "\n",
    "## Assignment Overview\n",
    "This notebook focuses on preparing and engineering features for predicting credit card defaults using a **cost-aware, fair, and explainable** machine learning approach.\n",
    "\n",
    "**Dataset**: [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset) from Kaggle\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand credit card default data characteristics\n",
    "- Perform thorough data quality assessment\n",
    "- Create meaningful features for ML modeling\n",
    "- Prepare data for fair and explainable ML approaches\n",
    "- Analyze potential fairness concerns in financial data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "### Dataset Information\n",
    "The dataset contains information on default payments, demographic factors, credit data, payment history, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n",
    "\n",
    "**To download the dataset:**\n",
    "1. Visit: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset\n",
    "2. Download `UCI_Credit_Card.csv`\n",
    "3. Place it in the same directory as this notebook or update the path below\n",
    "\n",
    "**Alternative**: You can also download it programmatically using the Kaggle API (requires setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Update this path if your data is located elsewhere\n",
    "data_path = 'UCI_Credit_Card.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úì Dataset loaded successfully!\")\n",
    "    print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found. Please download the dataset from Kaggle and place it in the correct location.\")\n",
    "    print(\"   Expected path:\", data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Features\n",
    "\n",
    "**Target Variable:**\n",
    "- `default.payment.next.month`: Default payment (1=yes, 0=no)\n",
    "\n",
    "**Demographic Features:**\n",
    "- `ID`: Customer ID\n",
    "- `SEX`: Gender (1=male, 2=female)\n",
    "- `EDUCATION`: Education level (1=graduate school, 2=university, 3=high school, 4=others)\n",
    "- `MARRIAGE`: Marital status (1=married, 2=single, 3=others)\n",
    "- `AGE`: Age in years\n",
    "\n",
    "**Credit Information:**\n",
    "- `LIMIT_BAL`: Amount of given credit (NT dollar)\n",
    "\n",
    "**Payment History (Sep 2005 - Apr 2005):**\n",
    "- `PAY_0` to `PAY_6`: Repayment status for each month\n",
    "  - -1=pay duly, 1=payment delay for one month, 2=payment delay for two months, etc.\n",
    "\n",
    "**Bill Statements (Sep 2005 - Apr 2005):**\n",
    "- `BILL_AMT1` to `BILL_AMT6`: Bill statement amount for each month\n",
    "\n",
    "**Payment Amounts (Sep 2005 - Apr 2005):**\n",
    "- `PAY_AMT1` to `PAY_AMT6`: Previous payment amount for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "print(\"Column Names:\")\n",
    "print(\"=\" * 80)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = 100 * df.isnull().sum() / len(df)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_table = missing_table[missing_table['Missing Count'] > 0].sort_values(\n",
    "    'Missing Count', ascending=False\n",
    ")\n",
    "\n",
    "if len(missing_table) == 0:\n",
    "    print(\"‚úì No missing values found!\")\n",
    "else:\n",
    "    print(missing_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate Records: {duplicates}\")\n",
    "\n",
    "# Check for duplicate IDs\n",
    "duplicate_ids = df['ID'].duplicated().sum()\n",
    "print(f\"Duplicate IDs: {duplicate_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(\"=\" * 80)\n",
    "target_col = 'default.payment.next.month'\n",
    "target_counts = df[target_col].value_counts().sort_index()\n",
    "target_pct = df[target_col].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_pct\n",
    "})\n",
    "target_summary.index = ['No Default (0)', 'Default (1)']\n",
    "print(target_summary)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "target_counts.plot(kind='bar', ax=ax[0], color=['#2ecc71', '#e74c3c'])\n",
    "ax[0].set_title('Target Variable Distribution', fontsize=12, fontweight='bold')\n",
    "ax[0].set_xlabel('Default Status')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_xticklabels(['No Default', 'Default'], rotation=0)\n",
    "\n",
    "ax[1].pie(target_counts, labels=['No Default', 'Default'], autopct='%1.1f%%',\n",
    "          colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "ax[1].set_title('Target Variable Proportion', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"‚ö†Ô∏è  Dataset is imbalanced. Consider using techniques like SMOTE, class weights, or stratified sampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Rename target variable for easier access\n",
    "df_clean = df_clean.rename(columns={'default.payment.next.month': 'default'})\n",
    "\n",
    "print(\"‚úì Dataset copied and target variable renamed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine categorical variables\n",
    "print(\"Categorical Variable Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_cols = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df_clean[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean EDUCATION variable (0, 5, 6 should be combined with 4 'others')\n",
    "print(\"Cleaning EDUCATION variable...\")\n",
    "print(f\"Before: {df_clean['EDUCATION'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "df_clean['EDUCATION'] = df_clean['EDUCATION'].replace({0: 4, 5: 4, 6: 4})\n",
    "\n",
    "print(f\"After: {df_clean['EDUCATION'].value_counts().sort_index().to_dict()}\")\n",
    "print(\"‚úì EDUCATION cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean MARRIAGE variable (0 should be combined with 3 'others')\n",
    "print(\"Cleaning MARRIAGE variable...\")\n",
    "print(f\"Before: {df_clean['MARRIAGE'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "df_clean['MARRIAGE'] = df_clean['MARRIAGE'].replace({0: 3})\n",
    "\n",
    "print(f\"After: {df_clean['MARRIAGE'].value_counts().sort_index().to_dict()}\")\n",
    "print(\"‚úì MARRIAGE cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine payment status variables\n",
    "print(\"Payment Status Variables Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "for col in pay_cols:\n",
    "    print(f\"\\n{col} unique values: {sorted(df_clean[col].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in numerical columns\n",
    "print(\"Outlier Detection (using IQR method):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numerical_cols = ['LIMIT_BAL', 'AGE'] + [f'BILL_AMT{i}' for i in range(1, 7)] + [f'PAY_AMT{i}' for i in range(1, 7)]\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)][col]\n",
    "    outlier_pct = (len(outliers) / len(df_clean)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outlier Count': len(outliers),\n",
    "        'Percentage': outlier_pct\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df[outlier_df['Outlier Count'] > 0].sort_values('Outlier Count', ascending=False)\n",
    "print(outlier_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: Outliers in financial data may represent genuine high-value transactions.\")\n",
    "print(\"   We'll keep them for now but may need to handle them during modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Demographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall age distribution\n",
    "axes[0].hist(df_clean['AGE'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Age Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(df_clean['AGE'].mean(), color='red', linestyle='--', label=f'Mean: {df_clean[\"AGE\"].mean():.1f}')\n",
    "axes[0].axvline(df_clean['AGE'].median(), color='green', linestyle='--', label=f'Median: {df_clean[\"AGE\"].median():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Age distribution by default status\n",
    "df_clean[df_clean['default'] == 0]['AGE'].hist(bins=30, alpha=0.6, label='No Default', ax=axes[1])\n",
    "df_clean[df_clean['default'] == 1]['AGE'].hist(bins=30, alpha=0.6, label='Default', ax=axes[1])\n",
    "axes[1].set_title('Age Distribution by Default Status', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender, Education, and Marriage analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# SEX\n",
    "sex_default = pd.crosstab(df_clean['SEX'], df_clean['default'], normalize='index') * 100\n",
    "sex_default.plot(kind='bar', ax=axes[0, 0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0, 0].set_title('Default Rate by Gender', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Gender (1=Male, 2=Female)')\n",
    "axes[0, 0].set_ylabel('Percentage')\n",
    "axes[0, 0].set_xticklabels(['Male', 'Female'], rotation=0)\n",
    "axes[0, 0].legend(['No Default', 'Default'])\n",
    "\n",
    "sex_counts = df_clean['SEX'].value_counts().sort_index()\n",
    "axes[1, 0].bar(['Male', 'Female'], sex_counts.values, color=['#3498db', '#e91e63'])\n",
    "axes[1, 0].set_title('Gender Distribution', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# EDUCATION\n",
    "edu_default = pd.crosstab(df_clean['EDUCATION'], df_clean['default'], normalize='index') * 100\n",
    "edu_default.plot(kind='bar', ax=axes[0, 1], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0, 1].set_title('Default Rate by Education', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Education Level')\n",
    "axes[0, 1].set_ylabel('Percentage')\n",
    "axes[0, 1].set_xticklabels(['Grad School', 'University', 'High School', 'Others'], rotation=45)\n",
    "axes[0, 1].legend(['No Default', 'Default'])\n",
    "\n",
    "edu_counts = df_clean['EDUCATION'].value_counts().sort_index()\n",
    "axes[1, 1].bar(['Grad School', 'University', 'High School', 'Others'], edu_counts.values)\n",
    "axes[1, 1].set_title('Education Distribution', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MARRIAGE\n",
    "mar_default = pd.crosstab(df_clean['MARRIAGE'], df_clean['default'], normalize='index') * 100\n",
    "mar_default.plot(kind='bar', ax=axes[0, 2], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0, 2].set_title('Default Rate by Marital Status', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Marital Status')\n",
    "axes[0, 2].set_ylabel('Percentage')\n",
    "axes[0, 2].set_xticklabels(['Married', 'Single', 'Others'], rotation=0)\n",
    "axes[0, 2].legend(['No Default', 'Default'])\n",
    "\n",
    "mar_counts = df_clean['MARRIAGE'].value_counts().sort_index()\n",
    "axes[1, 2].bar(['Married', 'Single', 'Others'], mar_counts.values, color=['#9b59b6', '#f39c12', '#34495e'])\n",
    "axes[1, 2].set_title('Marital Status Distribution', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Credit Limit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit limit analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of credit limit\n",
    "axes[0].hist(df_clean['LIMIT_BAL'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Credit Limit Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Credit Limit (NT$)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(df_clean['LIMIT_BAL'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: ${df_clean[\"LIMIT_BAL\"].mean():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Credit limit by default status\n",
    "df_clean.boxplot(column='LIMIT_BAL', by='default', ax=axes[1])\n",
    "axes[1].set_title('Credit Limit by Default Status', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Default Status (0=No, 1=Yes)')\n",
    "axes[1].set_ylabel('Credit Limit (NT$)')\n",
    "plt.suptitle('')  # Remove the automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"Credit Limit Statistics by Default Status:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_clean.groupby('default')['LIMIT_BAL'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Payment History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment status analysis\n",
    "pay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(pay_cols):\n",
    "    pay_default = pd.crosstab(df_clean[col], df_clean['default'], normalize='index') * 100\n",
    "    pay_default[1].plot(kind='bar', ax=axes[i], color='#e74c3c')\n",
    "    axes[i].set_title(f'Default Rate by {col}', fontsize=10, fontweight='bold')\n",
    "    axes[i].set_xlabel('Payment Status')\n",
    "    axes[i].set_ylabel('Default Rate (%)')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average payment status by default\n",
    "avg_pay_status = df_clean.groupby('default')[pay_cols].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "avg_pay_status.T.plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'])\n",
    "ax.set_title('Average Payment Status by Default Status', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Average Payment Status')\n",
    "ax.legend(['No Default', 'Default'])\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Bill Amount and Payment Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average bill amounts over time\n",
    "bill_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\n",
    "pay_amt_cols = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bill amounts\n",
    "avg_bill = df_clean.groupby('default')[bill_cols].mean()\n",
    "avg_bill.T.plot(ax=axes[0], marker='o', linewidth=2)\n",
    "axes[0].set_title('Average Bill Amount Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Month (1=Sep, 6=Apr)')\n",
    "axes[0].set_ylabel('Average Bill Amount (NT$)')\n",
    "axes[0].legend(['No Default', 'Default'])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Payment amounts\n",
    "avg_pay = df_clean.groupby('default')[pay_amt_cols].mean()\n",
    "avg_pay.T.plot(ax=axes[1], marker='o', linewidth=2)\n",
    "axes[1].set_title('Average Payment Amount Over Time', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Month (1=Sep, 6=Apr)')\n",
    "axes[1].set_ylabel('Average Payment Amount (NT$)')\n",
    "axes[1].legend(['No Default', 'Default'])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for key features\n",
    "key_features = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', \n",
    "                'BILL_AMT1', 'BILL_AMT2', 'PAY_AMT1', 'PAY_AMT2', 'default']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_clean[key_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Matrix - Key Features', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with default\n",
    "print(\"\\nFeatures Most Correlated with Default:\")\n",
    "print(\"=\" * 80)\n",
    "default_corr = correlation_matrix['default'].sort_values(ascending=False)\n",
    "print(default_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Payment Behavior Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_featured = df_clean.copy()\n",
    "\n",
    "print(\"Creating payment behavior features...\")\n",
    "\n",
    "# 1. Average payment status (higher = more delays)\n",
    "df_featured['avg_payment_status'] = df_featured[pay_cols].mean(axis=1)\n",
    "\n",
    "# 2. Maximum payment delay\n",
    "df_featured['max_payment_delay'] = df_featured[pay_cols].max(axis=1)\n",
    "\n",
    "# 3. Count of months with payment delay (payment status > 0)\n",
    "df_featured['num_delayed_payments'] = (df_featured[pay_cols] > 0).sum(axis=1)\n",
    "\n",
    "# 4. Count of months with on-time payment (payment status <= 0)\n",
    "df_featured['num_ontime_payments'] = (df_featured[pay_cols] <= 0).sum(axis=1)\n",
    "\n",
    "# 5. Payment status trend (is it getting worse?)\n",
    "# Positive slope means getting worse over time\n",
    "df_featured['payment_status_trend'] = df_featured['PAY_0'] - df_featured['PAY_6']\n",
    "\n",
    "# 6. Recent payment behavior (last 2 months average)\n",
    "df_featured['recent_payment_status'] = df_featured[['PAY_0', 'PAY_2']].mean(axis=1)\n",
    "\n",
    "print(\"‚úì Payment behavior features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Credit Utilization Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating credit utilization features...\")\n",
    "\n",
    "# 7. Average bill amount\n",
    "df_featured['avg_bill_amt'] = df_featured[bill_cols].mean(axis=1)\n",
    "\n",
    "# 8. Maximum bill amount\n",
    "df_featured['max_bill_amt'] = df_featured[bill_cols].max(axis=1)\n",
    "\n",
    "# 9. Bill amount volatility (standard deviation)\n",
    "df_featured['bill_amt_volatility'] = df_featured[bill_cols].std(axis=1)\n",
    "\n",
    "# 10. Credit utilization ratio (avg bill / credit limit)\n",
    "df_featured['credit_utilization'] = df_featured['avg_bill_amt'] / df_featured['LIMIT_BAL']\n",
    "df_featured['credit_utilization'] = df_featured['credit_utilization'].clip(upper=2)  # Cap at 200%\n",
    "\n",
    "# 11. Recent credit utilization (last month)\n",
    "df_featured['recent_credit_utilization'] = df_featured['BILL_AMT1'] / df_featured['LIMIT_BAL']\n",
    "df_featured['recent_credit_utilization'] = df_featured['recent_credit_utilization'].clip(upper=2)\n",
    "\n",
    "# 12. Bill amount trend (increasing or decreasing?)\n",
    "df_featured['bill_amt_trend'] = df_featured['BILL_AMT1'] - df_featured['BILL_AMT6']\n",
    "\n",
    "print(\"‚úì Credit utilization features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Payment Capacity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating payment capacity features...\")\n",
    "\n",
    "# 13. Average payment amount\n",
    "df_featured['avg_payment_amt'] = df_featured[pay_amt_cols].mean(axis=1)\n",
    "\n",
    "# 14. Payment to bill ratio (how much of the bill is being paid?)\n",
    "# Higher is better (paying more of the bill)\n",
    "df_featured['payment_to_bill_ratio'] = df_featured['avg_payment_amt'] / (df_featured['avg_bill_amt'] + 1)\n",
    "df_featured['payment_to_bill_ratio'] = df_featured['payment_to_bill_ratio'].clip(upper=5)\n",
    "\n",
    "# 15. Recent payment to bill ratio\n",
    "df_featured['recent_payment_to_bill'] = df_featured['PAY_AMT1'] / (df_featured['BILL_AMT1'] + 1)\n",
    "df_featured['recent_payment_to_bill'] = df_featured['recent_payment_to_bill'].clip(upper=5)\n",
    "\n",
    "# 16. Payment amount volatility\n",
    "df_featured['payment_amt_volatility'] = df_featured[pay_amt_cols].std(axis=1)\n",
    "\n",
    "# 17. Count of zero payments (months with no payment)\n",
    "df_featured['num_zero_payments'] = (df_featured[pay_amt_cols] == 0).sum(axis=1)\n",
    "\n",
    "# 18. Payment consistency (inverse of coefficient of variation)\n",
    "payment_mean = df_featured[pay_amt_cols].mean(axis=1)\n",
    "payment_std = df_featured[pay_amt_cols].std(axis=1)\n",
    "df_featured['payment_consistency'] = 1 / (1 + payment_std / (payment_mean + 1))\n",
    "\n",
    "print(\"‚úì Payment capacity features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating categorical features...\")\n",
    "\n",
    "# Create meaningful labels for categorical variables\n",
    "# SEX\n",
    "df_featured['SEX_male'] = (df_featured['SEX'] == 1).astype(int)\n",
    "df_featured['SEX_female'] = (df_featured['SEX'] == 2).astype(int)\n",
    "\n",
    "# EDUCATION - One-hot encoding\n",
    "df_featured['EDU_graduate'] = (df_featured['EDUCATION'] == 1).astype(int)\n",
    "df_featured['EDU_university'] = (df_featured['EDUCATION'] == 2).astype(int)\n",
    "df_featured['EDU_high_school'] = (df_featured['EDUCATION'] == 3).astype(int)\n",
    "df_featured['EDU_others'] = (df_featured['EDUCATION'] == 4).astype(int)\n",
    "\n",
    "# MARRIAGE - One-hot encoding\n",
    "df_featured['MAR_married'] = (df_featured['MARRIAGE'] == 1).astype(int)\n",
    "df_featured['MAR_single'] = (df_featured['MARRIAGE'] == 2).astype(int)\n",
    "df_featured['MAR_others'] = (df_featured['MARRIAGE'] == 3).astype(int)\n",
    "\n",
    "print(\"‚úì Categorical features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Age-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating age-based features...\")\n",
    "\n",
    "# Age groups\n",
    "df_featured['age_group'] = pd.cut(df_featured['AGE'], \n",
    "                                   bins=[0, 25, 35, 45, 55, 100],\n",
    "                                   labels=['<25', '25-35', '35-45', '45-55', '55+'])\n",
    "\n",
    "# One-hot encode age groups\n",
    "age_dummies = pd.get_dummies(df_featured['age_group'], prefix='age')\n",
    "df_featured = pd.concat([df_featured, age_dummies], axis=1)\n",
    "\n",
    "# Credit limit per year of age (as a rough measure of financial capacity relative to age)\n",
    "df_featured['credit_per_age'] = df_featured['LIMIT_BAL'] / df_featured['AGE']\n",
    "\n",
    "print(\"‚úì Age-based features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# High utilization + payment delays (risky combination)\n",
    "df_featured['high_util_delayed'] = (\n",
    "    (df_featured['credit_utilization'] > 0.7) & \n",
    "    (df_featured['num_delayed_payments'] > 2)\n",
    ").astype(int)\n",
    "\n",
    "# Low payment ratio + high bills (another risky combination)\n",
    "df_featured['low_payment_high_bill'] = (\n",
    "    (df_featured['payment_to_bill_ratio'] < 0.1) & \n",
    "    (df_featured['avg_bill_amt'] > df_featured['avg_bill_amt'].median())\n",
    ").astype(int)\n",
    "\n",
    "print(\"‚úì Interaction features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all new features created\n",
    "original_features = set(df_clean.columns)\n",
    "new_features = [col for col in df_featured.columns if col not in original_features]\n",
    "\n",
    "print(f\"Total new features created: {len(new_features)}\")\n",
    "print(\"\\nNew Features:\")\n",
    "print(\"=\" * 80)\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for new numerical features\n",
    "new_numerical_features = [\n",
    "    'avg_payment_status', 'max_payment_delay', 'num_delayed_payments',\n",
    "    'avg_bill_amt', 'credit_utilization', 'payment_to_bill_ratio',\n",
    "    'avg_payment_amt', 'num_zero_payments'\n",
    "]\n",
    "\n",
    "print(\"\\nSummary Statistics for Key Engineered Features:\")\n",
    "print(\"=\" * 80)\n",
    "df_featured[new_numerical_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Analyze Feature Importance (Correlation with Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation of new features with target\n",
    "feature_correlations = df_featured[new_numerical_features + ['default']].corr()['default'].drop('default')\n",
    "feature_correlations = feature_correlations.sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation of Engineered Features with Default:\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_correlations)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_correlations.plot(kind='barh', color=plt.cm.RdYlGn_r(np.abs(feature_correlations)))\n",
    "plt.title('Feature Correlation with Default', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fairness Analysis Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Protected Attributes Analysis\n",
    "\n",
    "For fair ML, we need to understand how our model might treat different demographic groups. Common protected attributes in financial contexts include gender, age, and marital status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze default rates across protected groups\n",
    "print(\"Default Rates by Protected Attributes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# By Gender\n",
    "print(\"\\n1. By Gender:\")\n",
    "gender_default = df_featured.groupby('SEX')['default'].agg(['mean', 'count'])\n",
    "gender_default.index = ['Male', 'Female']\n",
    "gender_default.columns = ['Default Rate', 'Count']\n",
    "print(gender_default)\n",
    "\n",
    "# By Education\n",
    "print(\"\\n2. By Education:\")\n",
    "edu_default = df_featured.groupby('EDUCATION')['default'].agg(['mean', 'count'])\n",
    "edu_default.index = ['Graduate School', 'University', 'High School', 'Others']\n",
    "edu_default.columns = ['Default Rate', 'Count']\n",
    "print(edu_default)\n",
    "\n",
    "# By Marriage\n",
    "print(\"\\n3. By Marital Status:\")\n",
    "mar_default = df_featured.groupby('MARRIAGE')['default'].agg(['mean', 'count'])\n",
    "mar_default.index = ['Married', 'Single', 'Others']\n",
    "mar_default.columns = ['Default Rate', 'Count']\n",
    "print(mar_default)\n",
    "\n",
    "# By Age Group\n",
    "print(\"\\n4. By Age Group:\")\n",
    "age_default = df_featured.groupby('age_group')['default'].agg(['mean', 'count'])\n",
    "age_default.columns = ['Default Rate', 'Count']\n",
    "print(age_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fairness metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Gender\n",
    "gender_default.plot(kind='bar', y='Default Rate', ax=axes[0, 0], \n",
    "                    color=['#3498db', '#e91e63'], legend=False)\n",
    "axes[0, 0].set_title('Default Rate by Gender', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Default Rate')\n",
    "axes[0, 0].set_xticklabels(['Male', 'Female'], rotation=0)\n",
    "\n",
    "# Education\n",
    "edu_default.plot(kind='bar', y='Default Rate', ax=axes[0, 1], legend=False)\n",
    "axes[0, 1].set_title('Default Rate by Education', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Default Rate')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Marriage\n",
    "mar_default.plot(kind='bar', y='Default Rate', ax=axes[1, 0], \n",
    "                 color=['#9b59b6', '#f39c12', '#34495e'], legend=False)\n",
    "axes[1, 0].set_title('Default Rate by Marital Status', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Default Rate')\n",
    "axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Age Group\n",
    "age_default.plot(kind='bar', y='Default Rate', ax=axes[1, 1], legend=False)\n",
    "axes[1, 1].set_title('Default Rate by Age Group', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Default Rate')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests for independence\n",
    "print(\"Chi-Square Tests for Independence:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "protected_attrs = [\n",
    "    ('SEX', 'Gender'),\n",
    "    ('EDUCATION', 'Education'),\n",
    "    ('MARRIAGE', 'Marital Status')\n",
    "]\n",
    "\n",
    "for attr, name in protected_attrs:\n",
    "    contingency_table = pd.crosstab(df_featured[attr], df_featured['default'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"  Degrees of freedom: {dof}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"  ‚ö†Ô∏è  Significant association found (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ‚úì No significant association (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Disparate Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate disparate impact ratio (80% rule)\n",
    "# Ratio should be >= 0.8 to avoid disparate impact\n",
    "\n",
    "print(\"Disparate Impact Analysis (80% Rule):\")\n",
    "print(\"=\" * 80)\n",
    "print(\"A ratio < 0.8 suggests potential disparate impact\\n\")\n",
    "\n",
    "# Gender\n",
    "male_default_rate = df_featured[df_featured['SEX'] == 1]['default'].mean()\n",
    "female_default_rate = df_featured[df_featured['SEX'] == 2]['default'].mean()\n",
    "gender_di_ratio = min(male_default_rate, female_default_rate) / max(male_default_rate, female_default_rate)\n",
    "\n",
    "print(f\"Gender:\")\n",
    "print(f\"  Male default rate: {male_default_rate:.4f}\")\n",
    "print(f\"  Female default rate: {female_default_rate:.4f}\")\n",
    "print(f\"  Disparate Impact Ratio: {gender_di_ratio:.4f}\")\n",
    "if gender_di_ratio < 0.8:\n",
    "    print(f\"  ‚ö†Ô∏è  Potential disparate impact detected!\")\n",
    "else:\n",
    "    print(f\"  ‚úì No disparate impact\")\n",
    "\n",
    "# Age groups\n",
    "print(f\"\\nAge Groups:\")\n",
    "age_group_rates = df_featured.groupby('age_group')['default'].mean()\n",
    "age_di_ratio = age_group_rates.min() / age_group_rates.max()\n",
    "print(age_group_rates)\n",
    "print(f\"  Disparate Impact Ratio: {age_di_ratio:.4f}\")\n",
    "if age_di_ratio < 0.8:\n",
    "    print(f\"  ‚ö†Ô∏è  Potential disparate impact detected!\")\n",
    "else:\n",
    "    print(f\"  ‚úì No disparate impact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Final Dataset for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "print(\"Preparing final dataset for modeling...\")\n",
    "\n",
    "# Original features to keep\n",
    "original_to_keep = [\n",
    "    'LIMIT_BAL', 'AGE',\n",
    "    'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "    'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'\n",
    "]\n",
    "\n",
    "# Engineered features to keep\n",
    "engineered_to_keep = [\n",
    "    # Payment behavior\n",
    "    'avg_payment_status', 'max_payment_delay', 'num_delayed_payments',\n",
    "    'num_ontime_payments', 'payment_status_trend', 'recent_payment_status',\n",
    "    # Credit utilization\n",
    "    'avg_bill_amt', 'max_bill_amt', 'bill_amt_volatility',\n",
    "    'credit_utilization', 'recent_credit_utilization', 'bill_amt_trend',\n",
    "    # Payment capacity\n",
    "    'avg_payment_amt', 'payment_to_bill_ratio', 'recent_payment_to_bill',\n",
    "    'payment_amt_volatility', 'num_zero_payments', 'payment_consistency',\n",
    "    # Demographic (encoded)\n",
    "    'SEX_male', 'SEX_female',\n",
    "    'EDU_graduate', 'EDU_university', 'EDU_high_school', 'EDU_others',\n",
    "    'MAR_married', 'MAR_single', 'MAR_others',\n",
    "    # Age groups\n",
    "    'age_<25', 'age_25-35', 'age_35-45', 'age_45-55', 'age_55+',\n",
    "    'credit_per_age',\n",
    "    # Interactions\n",
    "    'high_util_delayed', 'low_payment_high_bill'\n",
    "]\n",
    "\n",
    "# Protected attributes (for fairness monitoring - not for training)\n",
    "protected_attributes = ['SEX', 'EDUCATION', 'MARRIAGE', 'age_group']\n",
    "\n",
    "# Target\n",
    "target = ['default']\n",
    "\n",
    "# Combine all\n",
    "all_features = original_to_keep + engineered_to_keep\n",
    "all_columns = ['ID'] + protected_attributes + all_features + target\n",
    "\n",
    "# Create final dataset\n",
    "df_final = df_featured[all_columns].copy()\n",
    "\n",
    "print(f\"‚úì Final dataset created\")\n",
    "print(f\"  Shape: {df_final.shape}\")\n",
    "print(f\"  Features: {len(all_features)}\")\n",
    "print(f\"  Protected attributes: {len(protected_attributes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataset info\n",
    "print(\"\\nFinal Dataset Information:\")\n",
    "print(\"=\" * 80)\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining missing values or infinities\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing = df_final.isnull().sum().sum()\n",
    "print(f\"Missing values: {missing}\")\n",
    "\n",
    "# Check for infinite values in numerical columns\n",
    "numerical_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "infinite_counts = np.isinf(df_final[numerical_cols]).sum().sum()\n",
    "print(f\"Infinite values: {infinite_counts}\")\n",
    "\n",
    "if missing == 0 and infinite_counts == 0:\n",
    "    print(\"\\n‚úì Data quality check passed!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Data quality issues detected. Please review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared dataset\n",
    "output_path = 'credit_card_default_prepared.csv'\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Prepared dataset saved to: {output_path}\")\n",
    "print(f\"  Rows: {df_final.shape[0]:,}\")\n",
    "print(f\"  Columns: {df_final.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data dictionary for reference\n",
    "data_dictionary = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Category': (\n",
    "        ['Credit'] * 1 + ['Demographic'] * 1 +\n",
    "        ['Payment History'] * 6 +\n",
    "        ['Bill Amount'] * 6 +\n",
    "        ['Payment Amount'] * 6 +\n",
    "        ['Engineered - Payment Behavior'] * 6 +\n",
    "        ['Engineered - Credit Utilization'] * 6 +\n",
    "        ['Engineered - Payment Capacity'] * 6 +\n",
    "        ['Engineered - Demographics'] * 11 +\n",
    "        ['Engineered - Age'] * 6 +\n",
    "        ['Engineered - Interaction'] * 2\n",
    "    )\n",
    "})\n",
    "\n",
    "# Save data dictionary\n",
    "dict_path = 'feature_dictionary.csv'\n",
    "data_dictionary.to_csv(dict_path, index=False)\n",
    "print(f\"\\n‚úì Feature dictionary saved to: {dict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA PREPARATION & FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  ‚Ä¢ Total samples: {df_final.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Total features: {len(all_features)}\")\n",
    "print(f\"  ‚Ä¢ Original features: {len(original_to_keep)}\")\n",
    "print(f\"  ‚Ä¢ Engineered features: {len(engineered_to_keep)}\")\n",
    "print(f\"  ‚Ä¢ Protected attributes tracked: {len(protected_attributes)}\")\n",
    "\n",
    "print(\"\\nüéØ Target Variable:\")\n",
    "print(f\"  ‚Ä¢ Class distribution:\")\n",
    "print(f\"    - No default: {(df_final['default']==0).sum():,} ({(df_final['default']==0).mean()*100:.1f}%)\")\n",
    "print(f\"    - Default: {(df_final['default']==1).sum():,} ({(df_final['default']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è Fairness Considerations:\")\n",
    "print(f\"  ‚Ä¢ Protected attributes identified and analyzed\")\n",
    "print(f\"  ‚Ä¢ Disparate impact ratios calculated\")\n",
    "print(f\"  ‚Ä¢ Statistical significance tests performed\")\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "print(f\"  ‚Ä¢ Prepared dataset: {output_path}\")\n",
    "print(f\"  ‚Ä¢ Feature dictionary: {dict_path}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. Build baseline classical ML models (Logistic Regression, Random Forest, XGBoost)\")\n",
    "print(\"  2. Implement cost-sensitive learning (accounting for false negative/positive costs)\")\n",
    "print(\"  3. Apply fairness-aware algorithms and mitigation strategies\")\n",
    "print(\"  4. Develop deep learning models with interpretability\")\n",
    "print(\"  5. Use SHAP/LIME for model explainability\")\n",
    "print(\"  6. Evaluate models on both performance and fairness metrics\")\n",
    "print(\"  7. Compare cost-aware vs standard approaches\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights from Data Preparation\n",
    "\n",
    "### 1. **Class Imbalance**\n",
    "   - The dataset shows class imbalance favoring non-defaulters\n",
    "   - Will require techniques like SMOTE, class weights, or cost-sensitive learning\n",
    "\n",
    "### 2. **Feature Engineering Impact**\n",
    "   - Created 30+ engineered features capturing:\n",
    "     - Payment behavior patterns\n",
    "     - Credit utilization trends\n",
    "     - Payment capacity indicators\n",
    "   - These features show stronger correlation with default than many raw features\n",
    "\n",
    "### 3. **Fairness Considerations**\n",
    "   - Identified potential disparate impact across demographic groups\n",
    "   - Protected attributes (gender, age, education) will need careful monitoring\n",
    "   - Must ensure models don't discriminate based on these attributes\n",
    "\n",
    "### 4. **Data Quality**\n",
    "   - No missing values in dataset\n",
    "   - Clean categorical variables\n",
    "   - Outliers present but may represent genuine cases\n",
    "\n",
    "### 5. **Cost-Aware Considerations**\n",
    "   - False negatives (missing defaults) are more costly than false positives\n",
    "   - Will need to adjust decision thresholds and use appropriate cost matrices\n",
    "   - Business impact should drive model evaluation metrics\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for modeling!** The prepared dataset is now suitable for building cost-aware, fair, and explainable ML models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
